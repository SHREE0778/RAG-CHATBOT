# RAG-CHATBOT
RAG-CHATBOT

A Streamlit-based Retrieval-Augmented Generation assistant with document upload, local embeddings, user login, and per-user chat history.

ğŸš€ Overview

RAG-CHATBOT is a fully local, privacy-friendly Retrieval-Augmented Generation (RAG) system built using Streamlit, ChromaDB, and Groq LLaMA Models (or any open-source LLM).
It allows users to:

Upload documents (PDF, TXT)

Generate embeddings locally

Chat with an AI assistant grounded on the uploaded knowledge

Maintain separate chat history per user

Authenticate using a secure login system

This project is ideal for students, researchers, enterprise users, and anyone building production-grade RAG applications.

âœ¨ Features
ğŸ” User Authentication

Email + password login

Secure SHA-256 password hashing

User-specific session handling

ğŸ’¬ Chat History (Per User)

Each user sees only their own chat history

Messages stored in SQLite database

Persists across reloads

ğŸ“„ Document Upload

Upload PDFs/TXT files

Automatic text extraction

Chunking & embedding generation

ğŸ§  Local Vector Search

Uses ChromaDB for embeddings

Fast semantic search

No external dependencies required

ğŸ¤– RAG Pipeline

Retrieves top relevant chunks

Passes context â†’ LLM

Produces accurate, grounded answers

ğŸ–¥ï¸ Streamlit UI

Clean modern interface

Chat-style conversation

Supports real-time retrieval & generation

ğŸ—‚ Project Structure
RAG-CHATBOT/
â”‚
â”œâ”€â”€ app.py                # Streamlit main application (UI + Chat)
â”œâ”€â”€ ingest.py             # Document ingestion + embedding creation
â”œâ”€â”€ db.py                 # Chat history database (SQLite)
â”œâ”€â”€ auth.py               # User login + signup system
â”œâ”€â”€ config.py             # Model/API configuration
â”‚
â”œâ”€â”€ docs/chroma/          # Local ChromaDB vector store
â”‚
â”œâ”€â”€ README.md             # Project documentation
â”œâ”€â”€ pyproject.toml        # Project dependencies
â””â”€â”€ uv.lock               # Dependency lock file

ğŸ”§ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR_USERNAME/RAG-CHATBOT.git
cd RAG-CHATBOT

2ï¸âƒ£ Install dependencies

Using uv or pip:

pip install -r requirements.txt


or

uv sync

3ï¸âƒ£ Run ingestion (optional)
python ingest.py

4ï¸âƒ£ Start the Streamlit app
streamlit run app.py

ğŸ§© How It Works
1. Upload Documents

Users upload PDF/TXT files â†’ text is extracted â†’ chunked â†’ embeddings generated.

2. Store Embeddings Locally

ChromaDB stores embeddings in docs/chroma/.

3. Retrieve Relevant Chunks

User query â†’ semantic search â†’ top matching chunks returned.

4. Generate Answer

Query + retrieved context â†’ sent to LLM â†’ final grounded answer displayed.

ğŸ” Authentication & User Data

User credentials stored securely (hashed, not plaintext)

Chat history separated by user email

SQLite ensures fast and local storage

No data sent to third-party servers (unless you use cloud LLMs)

ğŸ¤– Models Used

You can use:

Groq Models (Recommended)

LLaMA-3 8B

Mixtral

Gemma

Or local models such as:

LLaMA-3 (GGUF)

Mistral

Qwen

ğŸ§ª Example Query

User: "Summarize the key points from the uploaded PDF."
Bot: Provides a context-grounded summary using retrieved chunks.

ğŸ“Œ Future Enhancements

Admin dashboard

JWT-based authentication

Multiple document workspaces per user

Vector store migrations

Response citations

PDF preview UI

ğŸ§‘â€ğŸ’» Contributing

Contributions are welcome!
Feel free to submit issues or pull requests.

ğŸ“œ License

This project is released under the MIT License.

â­ Show Your Support

If you found this project useful, please star the repository on GitHub â¤ï¸
